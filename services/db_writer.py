"""
æ•°æ®åº“å¼‚æ­¥æ‰¹é‡å†™å…¥å®ˆæŠ¤è¿›ç¨‹ (Write-Behind Daemon)
åŠŸèƒ½ï¼šä» Redis é˜Ÿåˆ—æå–æ•°æ® -> æ‰¹é‡å†™å…¥ SQLite
"""
import time
import json
import logging
import traceback
from typing import List, Dict, Any

import redis
from sqlalchemy import text

# å¼•å…¥ä½ çš„æ•°æ®åº“å·¥å…·
from database.factory import get_db, begin_tx, commit_tx
import settings
from celery_app import RedisKeys

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("DB_WRITER")

# è¿æ¥ Redis
redis_client = redis.from_url(settings.REDIS_URL)

BATCH_SIZE = 500  # æ¯æ¬¡æ‰¹é‡å†™å…¥çš„æ¡æ•°
FLUSH_INTERVAL = 2.0  # æœ€é•¿ç­‰å¾…æ—¶é—´ (ç§’)ï¼Œé˜²æ­¢æ•°æ®ç§¯å‹å¤ªä¹…


def run_db_writer():
    logger.info("ğŸš€ DB Writer Daemon started... Waiting for data.")

    pending_items = []
    last_flush_time = time.time()

    while True:
        try:
            # 1. éé˜»å¡æå–æ•°æ®
            # RPOP ç›¸æ¯” BLPOP æ›´å®¹æ˜“æ§åˆ¶ flush é—´éš”
            raw_data = redis_client.rpop(RedisKeys.DB_WRITE_QUEUE)

            if raw_data:
                pending_items.append(raw_data)
            else:
                # é˜Ÿåˆ—ç©ºäº†ï¼Œä¼‘æ¯ä¸€ä¸‹é¿å… CPU 100%
                time.sleep(0.1)

            # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘å†™å…¥ (æ•°é‡å¤Ÿäº† OR æ—¶é—´åˆ°äº†)
            current_time = time.time()
            is_batch_full = len(pending_items) >= BATCH_SIZE
            is_timeout = (len(pending_items) > 0) and (current_time - last_flush_time >= FLUSH_INTERVAL)

            if is_batch_full or is_timeout:
                _flush_buffer(pending_items)
                pending_items = []  # æ¸…ç©ºç¼“å†²åŒº
                last_flush_time = current_time

        except Exception as e:
            logger.error(f"Critical Loop Error: {e}")
            time.sleep(5)  # å‡ºé”™åå†·å´


def _flush_buffer(raw_items: List[bytes]):
    """æ‰§è¡Œæ‰¹é‡å†™å…¥é€»è¾‘"""
    if not raw_items:
        return

    # æŒ‰è¡¨åˆ†ç»„æ•°æ®
    # ç»“æ„: { "mail_message": [dict1, dict2], "mail_body": [...] }
    grouped_data: Dict[str, List[Dict[str, Any]]] = {}

    # ä¸´æ—¶ä¿å­˜è§£æå¤±è´¥çš„ item ä»¥ä¾¿é‡è¯•ï¼ˆå¯é€‰ï¼‰
    failed_items = []

    for raw in raw_items:
        try:
            # æ•°æ®åè®®: {"table": "table_name", "data": {...}}
            payload = json.loads(raw)
            table_name = payload.get("table")
            row_data = payload.get("data")

            if table_name and row_data:
                if table_name not in grouped_data:
                    grouped_data[table_name] = []
                grouped_data[table_name].append(row_data)
        except Exception:
            logger.error("Failed to parse JSON item, discarding.")
            continue

    if not grouped_data:
        return

    # å¼€å§‹æ•°æ®åº“äº‹åŠ¡
    try:
        start_t = time.time()
        with get_db() as db:
            begin_tx(db)
            total_records = 0
            for table, rows in grouped_data.items():
                if not rows:
                    continue
                keys = list(rows[0].keys())
                columns = ", ".join(keys)
                placeholders = ", ".join(["?" for _ in keys])
                action = "INSERT OR REPLACE" if table == "mail_body" else "INSERT OR IGNORE"
                sql = f"{action} INTO {table} ({columns}) VALUES ({placeholders})"
                values_list = []
                for row in rows:
                    values_list.append(tuple(row[k] for k in keys))
                db.executemany(sql, values_list)
                total_records += len(rows)

            # E. æäº¤äº‹åŠ¡
            commit_tx(db)

            duration = time.time() - start_t
            logger.info(f"âœ… Flushed {total_records} records (Tables: {list(grouped_data.keys())}) in {duration:.3f}s")

    except Exception as e:
        logger.error(f"âŒ DB Write Failed: {e}")
        # ç´§æ€¥é¿é™©ï¼šå°†æ•°æ®å¡å› Redis é˜Ÿåˆ—å¤´éƒ¨ (Lpush)ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±
        # æ³¨æ„ï¼šè¿™å¯èƒ½ä¼šå¯¼è‡´æ­»å¾ªç¯å¦‚æœæ•°æ®æœ¬èº«æœ‰é—®é¢˜ï¼Œç”Ÿäº§ç¯å¢ƒéœ€è¦ Dead Letter Queue (æ­»ä¿¡é˜Ÿåˆ—)
        logger.warning(f"Re-queuing {len(raw_items)} items...")

        pipe = redis_client.pipeline()
        for item in raw_items:
            pipe.lpush(RedisKeys.DB_WRITE_QUEUE, item)
        pipe.execute()


if __name__ == "__main__":
    # å¯ä»¥ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶å¯åŠ¨å®ˆæŠ¤è¿›ç¨‹
    run_db_writer()